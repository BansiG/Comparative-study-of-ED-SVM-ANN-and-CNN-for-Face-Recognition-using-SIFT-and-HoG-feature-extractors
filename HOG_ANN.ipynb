{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\bansi\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from skimage.feature import hog as hg\n",
    "from skimage import data, exposure\n",
    "import cv2\n",
    "import os\n",
    "from sklearn.svm import SVC  \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from PIL import Image\n",
    "from sklearn.decomposition import PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17440004\n",
      "1744001\n",
      "1744002\n",
      "1744003\n",
      "1849004\n",
      "201501004\n",
      "201501007\n",
      "201501008\n",
      "201501009\n",
      "201501011\n",
      "201501012\n",
      "201501021\n",
      "201501025\n",
      "201501028\n",
      "201501031\n",
      "201501032\n",
      "201501034\n",
      "201501038\n",
      "201501039\n",
      "201501051\n",
      "201501053\n",
      "201501054\n",
      "201501055\n",
      "201501060\n",
      "201501067\n",
      "201501070\n",
      "201501071\n",
      "201501077\n",
      "201501079\n",
      "201501086\n",
      "201501088\n",
      "201501091\n",
      "201501095\n",
      "201501097\n",
      "201501101\n",
      "201501104\n",
      "201501109\n",
      "201501112\n",
      "201501121\n",
      "20180119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\bansi\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\skimage\\feature\\_hog.py:119: skimage_deprecation: Default value of `block_norm`==`L1` is deprecated and will be changed to `L2-Hys` in v0.15\n",
      "  'be changed to `L2-Hys` in v0.15', skimage_deprecation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\n",
      "(386, 11560)\n",
      "(483,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "image_array = []\n",
    "        \n",
    "features = []\n",
    "label = []\n",
    "unqiue_label = []\n",
    "for files in os.listdir('F:/Desktop/ICT/sem_6/ML/train/new_faces'):\n",
    "    f = files.split('_')[0].split('O')[0].split('o')[0]\n",
    "    if(f!='.DS'):\n",
    "        image_array.append(cv2.imread('F:/Desktop/ICT/sem_6/ML/train/new_faces/'+files , 0))\n",
    "\n",
    "        label.append(f);\n",
    "        if( f not in unqiue_label ):\n",
    "            print(f)\n",
    "            unqiue_label.append(f)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split( image_array, label, test_size=0.2)\n",
    "\n",
    "for image in X_train:\n",
    "            \n",
    "    fd, hog_image = hg(image, orientations=10, pixels_per_cell=(16, 16),\n",
    "                        cells_per_block=(2, 2), visualise= True)\n",
    "    features.append(fd)\n",
    "        \n",
    "features = np.array(features)\n",
    "features = features\n",
    "print(\"data\")\n",
    "print(features.shape)\n",
    "print(np.array(label).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(386, 11560)\n",
      "(386, 50)\n",
      "0.5838204977044017\n"
     ]
    }
   ],
   "source": [
    "print(features.shape)\n",
    "\n",
    "pca = PCA(n_components=50)\n",
    "pca.fit(features)\n",
    "\n",
    "\n",
    "x = pca.transform(features)\n",
    "print(x.shape)\n",
    "\n",
    "label = np.array(y_train)\n",
    "\n",
    "print(np.sum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(386, 50)\n",
      "(50,)\n"
     ]
    }
   ],
   "source": [
    "new_x = x/100\n",
    "print(new_x.shape)\n",
    "y = []\n",
    "\n",
    "for label_ in label:\n",
    "    onel = np.zeros(40,dtype=int)\n",
    "    np.put(onel,unqiue_label.index(label_),1)\n",
    "    y.append(onel)\n",
    "    \n",
    "y = np.array(y)\n",
    "print(x[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_layer (InputLayer)     (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "layer_1 (Dense)              (None, 128)               6528      \n",
      "_________________________________________________________________\n",
      "layer_2 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "output_layer (Dense)         (None, 40)                5160      \n",
      "=================================================================\n",
      "Total params: 28,200\n",
      "Trainable params: 28,200\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dropout,Input\n",
    "from keras.models import Model\n",
    "\n",
    "input_feature_vector = Input(shape=(50,),name='input_layer')\n",
    "X = Dense(128, activation='relu',name= 'layer_1')(input_feature_vector)\n",
    "hidden1 = Dense(128,activation='relu',name= 'layer_2')(X)\n",
    "Y = Dense(40,activation= 'softmax',name= 'output_layer')(hidden1)\n",
    "\n",
    "model = Model(input_feature_vector, Y)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "adam = optimizers.Adam(lr=0.001)\n",
    "\n",
    "model.compile(optimizer=adam,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "386/386 [==============================] - 0s 93us/step - loss: 0.8665 - acc: 0.7953\n",
      "Epoch 2/120\n",
      "386/386 [==============================] - 0s 73us/step - loss: 0.8618 - acc: 0.7953\n",
      "Epoch 3/120\n",
      "386/386 [==============================] - 0s 63us/step - loss: 0.8545 - acc: 0.7979\n",
      "Epoch 4/120\n",
      "386/386 [==============================] - 0s 81us/step - loss: 0.8551 - acc: 0.7979\n",
      "Epoch 5/120\n",
      "386/386 [==============================] - 0s 88us/step - loss: 0.8543 - acc: 0.8057\n",
      "Epoch 6/120\n",
      "386/386 [==============================] - 0s 83us/step - loss: 0.8448 - acc: 0.7979\n",
      "Epoch 7/120\n",
      "386/386 [==============================] - 0s 83us/step - loss: 0.8453 - acc: 0.7927\n",
      "Epoch 8/120\n",
      "386/386 [==============================] - 0s 83us/step - loss: 0.8460 - acc: 0.7850\n",
      "Epoch 9/120\n",
      "386/386 [==============================] - 0s 83us/step - loss: 0.8339 - acc: 0.8031\n",
      "Epoch 10/120\n",
      "386/386 [==============================] - 0s 83us/step - loss: 0.8318 - acc: 0.8005\n",
      "Epoch 11/120\n",
      "386/386 [==============================] - 0s 83us/step - loss: 0.8286 - acc: 0.8187\n",
      "Epoch 12/120\n",
      "386/386 [==============================] - 0s 73us/step - loss: 0.8295 - acc: 0.8083\n",
      "Epoch 13/120\n",
      "386/386 [==============================] - 0s 83us/step - loss: 0.8226 - acc: 0.7953\n",
      "Epoch 14/120\n",
      "386/386 [==============================] - 0s 73us/step - loss: 0.8223 - acc: 0.8005\n",
      "Epoch 15/120\n",
      "386/386 [==============================] - 0s 90us/step - loss: 0.8171 - acc: 0.8135\n",
      "Epoch 16/120\n",
      "386/386 [==============================] - 0s 83us/step - loss: 0.8137 - acc: 0.8031\n",
      "Epoch 17/120\n",
      "386/386 [==============================] - 0s 93us/step - loss: 0.8113 - acc: 0.8135\n",
      "Epoch 18/120\n",
      "386/386 [==============================] - 0s 93us/step - loss: 0.8196 - acc: 0.7979\n",
      "Epoch 19/120\n",
      "386/386 [==============================] - 0s 83us/step - loss: 0.8169 - acc: 0.7927\n",
      "Epoch 20/120\n",
      "386/386 [==============================] - 0s 93us/step - loss: 0.8053 - acc: 0.8057\n",
      "Epoch 21/120\n",
      "386/386 [==============================] - 0s 104us/step - loss: 0.8021 - acc: 0.8135\n",
      "Epoch 22/120\n",
      "386/386 [==============================] - 0s 83us/step - loss: 0.8005 - acc: 0.8212\n",
      "Epoch 23/120\n",
      "386/386 [==============================] - 0s 104us/step - loss: 0.7992 - acc: 0.8109\n",
      "Epoch 24/120\n",
      "386/386 [==============================] - 0s 83us/step - loss: 0.7934 - acc: 0.8212\n",
      "Epoch 25/120\n",
      "386/386 [==============================] - 0s 83us/step - loss: 0.7961 - acc: 0.8083\n",
      "Epoch 26/120\n",
      "386/386 [==============================] - 0s 114us/step - loss: 0.7861 - acc: 0.8187\n",
      "Epoch 27/120\n",
      "386/386 [==============================] - 0s 93us/step - loss: 0.7875 - acc: 0.8083\n",
      "Epoch 28/120\n",
      "386/386 [==============================] - 0s 83us/step - loss: 0.7809 - acc: 0.8187\n",
      "Epoch 29/120\n",
      "386/386 [==============================] - 0s 104us/step - loss: 0.7786 - acc: 0.8290\n",
      "Epoch 30/120\n",
      "386/386 [==============================] - 0s 83us/step - loss: 0.7736 - acc: 0.8238\n",
      "Epoch 31/120\n",
      "386/386 [==============================] - 0s 83us/step - loss: 0.7736 - acc: 0.8212\n",
      "Epoch 32/120\n",
      "386/386 [==============================] - 0s 93us/step - loss: 0.7753 - acc: 0.8264\n",
      "Epoch 33/120\n",
      "386/386 [==============================] - 0s 83us/step - loss: 0.7717 - acc: 0.8238\n",
      "Epoch 34/120\n",
      "386/386 [==============================] - 0s 83us/step - loss: 0.7665 - acc: 0.8316\n",
      "Epoch 35/120\n",
      "386/386 [==============================] - 0s 93us/step - loss: 0.7657 - acc: 0.8109\n",
      "Epoch 36/120\n",
      "386/386 [==============================] - 0s 104us/step - loss: 0.7602 - acc: 0.8290\n",
      "Epoch 37/120\n",
      "386/386 [==============================] - 0s 83us/step - loss: 0.7564 - acc: 0.8316\n",
      "Epoch 38/120\n",
      "386/386 [==============================] - 0s 83us/step - loss: 0.7529 - acc: 0.8342\n",
      "Epoch 39/120\n",
      "386/386 [==============================] - 0s 124us/step - loss: 0.7509 - acc: 0.8290\n",
      "Epoch 40/120\n",
      "386/386 [==============================] - 0s 93us/step - loss: 0.7516 - acc: 0.8316\n",
      "Epoch 41/120\n",
      "386/386 [==============================] - 0s 83us/step - loss: 0.7459 - acc: 0.8342\n",
      "Epoch 42/120\n",
      "386/386 [==============================] - 0s 93us/step - loss: 0.7417 - acc: 0.8342\n",
      "Epoch 43/120\n",
      "386/386 [==============================] - 0s 124us/step - loss: 0.7438 - acc: 0.8212\n",
      "Epoch 44/120\n",
      "386/386 [==============================] - 0s 114us/step - loss: 0.7424 - acc: 0.8342\n",
      "Epoch 45/120\n",
      "386/386 [==============================] - 0s 114us/step - loss: 0.7392 - acc: 0.8264\n",
      "Epoch 46/120\n",
      "386/386 [==============================] - ETA: 0s - loss: 0.7698 - acc: 0.775 - 0s 83us/step - loss: 0.7361 - acc: 0.8342\n",
      "Epoch 47/120\n",
      "386/386 [==============================] - 0s 93us/step - loss: 0.7375 - acc: 0.8342\n",
      "Epoch 48/120\n",
      "386/386 [==============================] - 0s 93us/step - loss: 0.7291 - acc: 0.8394\n",
      "Epoch 49/120\n",
      "386/386 [==============================] - 0s 83us/step - loss: 0.7231 - acc: 0.8420\n",
      "Epoch 50/120\n",
      "386/386 [==============================] - 0s 93us/step - loss: 0.7214 - acc: 0.8420\n",
      "Epoch 51/120\n",
      "386/386 [==============================] - 0s 104us/step - loss: 0.7215 - acc: 0.8394\n",
      "Epoch 52/120\n",
      "386/386 [==============================] - 0s 83us/step - loss: 0.7188 - acc: 0.8394\n",
      "Epoch 53/120\n",
      "386/386 [==============================] - 0s 83us/step - loss: 0.7214 - acc: 0.8368\n",
      "Epoch 54/120\n",
      "386/386 [==============================] - 0s 83us/step - loss: 0.7115 - acc: 0.8316\n",
      "Epoch 55/120\n",
      "386/386 [==============================] - 0s 93us/step - loss: 0.7055 - acc: 0.8394\n",
      "Epoch 56/120\n",
      "386/386 [==============================] - 0s 83us/step - loss: 0.7086 - acc: 0.8472\n",
      "Epoch 57/120\n",
      "386/386 [==============================] - 0s 93us/step - loss: 0.7078 - acc: 0.8472\n",
      "Epoch 58/120\n",
      "386/386 [==============================] - 0s 83us/step - loss: 0.7055 - acc: 0.8472\n",
      "Epoch 59/120\n",
      "386/386 [==============================] - 0s 83us/step - loss: 0.7054 - acc: 0.8446\n",
      "Epoch 60/120\n",
      "386/386 [==============================] - 0s 104us/step - loss: 0.7031 - acc: 0.8446\n",
      "Epoch 61/120\n",
      "386/386 [==============================] - 0s 93us/step - loss: 0.6960 - acc: 0.8342\n",
      "Epoch 62/120\n",
      "386/386 [==============================] - 0s 93us/step - loss: 0.6945 - acc: 0.8472\n",
      "Epoch 63/120\n",
      "386/386 [==============================] - 0s 93us/step - loss: 0.6869 - acc: 0.8472\n",
      "Epoch 64/120\n",
      "386/386 [==============================] - 0s 83us/step - loss: 0.6879 - acc: 0.8497\n",
      "Epoch 65/120\n",
      "386/386 [==============================] - 0s 93us/step - loss: 0.6832 - acc: 0.8472\n",
      "Epoch 66/120\n",
      "386/386 [==============================] - 0s 114us/step - loss: 0.6815 - acc: 0.8497\n",
      "Epoch 67/120\n",
      "386/386 [==============================] - 0s 93us/step - loss: 0.6847 - acc: 0.8497\n",
      "Epoch 68/120\n",
      "386/386 [==============================] - 0s 104us/step - loss: 0.6778 - acc: 0.8523\n",
      "Epoch 69/120\n",
      "386/386 [==============================] - 0s 83us/step - loss: 0.6740 - acc: 0.8601\n",
      "Epoch 70/120\n",
      "386/386 [==============================] - 0s 93us/step - loss: 0.6695 - acc: 0.8497\n",
      "Epoch 71/120\n",
      "386/386 [==============================] - 0s 83us/step - loss: 0.6712 - acc: 0.8549\n",
      "Epoch 72/120\n",
      "386/386 [==============================] - 0s 104us/step - loss: 0.6646 - acc: 0.8627\n",
      "Epoch 73/120\n",
      "386/386 [==============================] - 0s 114us/step - loss: 0.6628 - acc: 0.8549\n",
      "Epoch 74/120\n",
      "386/386 [==============================] - 0s 93us/step - loss: 0.6627 - acc: 0.8497\n",
      "Epoch 75/120\n",
      "386/386 [==============================] - 0s 93us/step - loss: 0.6620 - acc: 0.8472\n",
      "Epoch 76/120\n",
      "386/386 [==============================] - 0s 83us/step - loss: 0.6598 - acc: 0.8679\n",
      "Epoch 77/120\n",
      "386/386 [==============================] - 0s 83us/step - loss: 0.6525 - acc: 0.8575\n",
      "Epoch 78/120\n",
      "386/386 [==============================] - 0s 83us/step - loss: 0.6570 - acc: 0.8497\n",
      "Epoch 79/120\n",
      "386/386 [==============================] - 0s 93us/step - loss: 0.6580 - acc: 0.8523\n",
      "Epoch 80/120\n",
      "386/386 [==============================] - 0s 83us/step - loss: 0.6520 - acc: 0.8523\n",
      "Epoch 81/120\n",
      "386/386 [==============================] - 0s 93us/step - loss: 0.6508 - acc: 0.8627\n",
      "Epoch 82/120\n",
      "386/386 [==============================] - 0s 93us/step - loss: 0.6460 - acc: 0.8575\n",
      "Epoch 83/120\n",
      "386/386 [==============================] - 0s 83us/step - loss: 0.6442 - acc: 0.8549\n",
      "Epoch 84/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "386/386 [==============================] - 0s 73us/step - loss: 0.6454 - acc: 0.8497\n",
      "Epoch 85/120\n",
      "386/386 [==============================] - 0s 83us/step - loss: 0.6373 - acc: 0.8549\n",
      "Epoch 86/120\n",
      "386/386 [==============================] - 0s 83us/step - loss: 0.6316 - acc: 0.8653\n",
      "Epoch 87/120\n",
      "386/386 [==============================] - 0s 73us/step - loss: 0.6307 - acc: 0.8653\n",
      "Epoch 88/120\n",
      "386/386 [==============================] - 0s 93us/step - loss: 0.6334 - acc: 0.8601\n",
      "Epoch 89/120\n",
      "386/386 [==============================] - 0s 83us/step - loss: 0.6346 - acc: 0.8679\n",
      "Epoch 90/120\n",
      "386/386 [==============================] - 0s 114us/step - loss: 0.6288 - acc: 0.8627\n",
      "Epoch 91/120\n",
      "386/386 [==============================] - 0s 93us/step - loss: 0.6195 - acc: 0.8653\n",
      "Epoch 92/120\n",
      "386/386 [==============================] - 0s 114us/step - loss: 0.6251 - acc: 0.8601\n",
      "Epoch 93/120\n",
      "386/386 [==============================] - 0s 93us/step - loss: 0.6191 - acc: 0.8601\n",
      "Epoch 94/120\n",
      "386/386 [==============================] - 0s 104us/step - loss: 0.6187 - acc: 0.8679\n",
      "Epoch 95/120\n",
      "386/386 [==============================] - 0s 93us/step - loss: 0.6125 - acc: 0.8731\n",
      "Epoch 96/120\n",
      "386/386 [==============================] - 0s 124us/step - loss: 0.6157 - acc: 0.8731\n",
      "Epoch 97/120\n",
      "386/386 [==============================] - 0s 104us/step - loss: 0.6095 - acc: 0.8782\n",
      "Epoch 98/120\n",
      "386/386 [==============================] - 0s 93us/step - loss: 0.6060 - acc: 0.8705\n",
      "Epoch 99/120\n",
      "386/386 [==============================] - 0s 104us/step - loss: 0.6078 - acc: 0.8627\n",
      "Epoch 100/120\n",
      "386/386 [==============================] - 0s 104us/step - loss: 0.6075 - acc: 0.8782\n",
      "Epoch 101/120\n",
      "386/386 [==============================] - 0s 176us/step - loss: 0.6004 - acc: 0.8601\n",
      "Epoch 102/120\n",
      "386/386 [==============================] - 0s 176us/step - loss: 0.5978 - acc: 0.8679\n",
      "Epoch 103/120\n",
      "386/386 [==============================] - 0s 104us/step - loss: 0.5957 - acc: 0.8653\n",
      "Epoch 104/120\n",
      "386/386 [==============================] - 0s 93us/step - loss: 0.5910 - acc: 0.8731\n",
      "Epoch 105/120\n",
      "386/386 [==============================] - 0s 124us/step - loss: 0.5918 - acc: 0.8679\n",
      "Epoch 106/120\n",
      "386/386 [==============================] - 0s 114us/step - loss: 0.5851 - acc: 0.8782\n",
      "Epoch 107/120\n",
      "386/386 [==============================] - 0s 93us/step - loss: 0.5851 - acc: 0.8808\n",
      "Epoch 108/120\n",
      "386/386 [==============================] - 0s 104us/step - loss: 0.5831 - acc: 0.8782\n",
      "Epoch 109/120\n",
      "386/386 [==============================] - 0s 104us/step - loss: 0.5836 - acc: 0.8808\n",
      "Epoch 110/120\n",
      "386/386 [==============================] - 0s 104us/step - loss: 0.5774 - acc: 0.8834\n",
      "Epoch 111/120\n",
      "386/386 [==============================] - 0s 104us/step - loss: 0.5811 - acc: 0.8834\n",
      "Epoch 112/120\n",
      "386/386 [==============================] - 0s 104us/step - loss: 0.5836 - acc: 0.8756\n",
      "Epoch 113/120\n",
      "386/386 [==============================] - 0s 104us/step - loss: 0.5747 - acc: 0.8705\n",
      "Epoch 114/120\n",
      "386/386 [==============================] - 0s 83us/step - loss: 0.5703 - acc: 0.8834\n",
      "Epoch 115/120\n",
      "386/386 [==============================] - 0s 93us/step - loss: 0.5720 - acc: 0.8808\n",
      "Epoch 116/120\n",
      "386/386 [==============================] - 0s 93us/step - loss: 0.5690 - acc: 0.8808\n",
      "Epoch 117/120\n",
      "386/386 [==============================] - 0s 93us/step - loss: 0.5713 - acc: 0.8756\n",
      "Epoch 118/120\n",
      "386/386 [==============================] - 0s 114us/step - loss: 0.5607 - acc: 0.8834\n",
      "Epoch 119/120\n",
      "386/386 [==============================] - 0s 93us/step - loss: 0.5584 - acc: 0.8808\n",
      "Epoch 120/120\n",
      "386/386 [==============================] - 0s 93us/step - loss: 0.5624 - acc: 0.8834\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x232345fa668>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(new_x, y, epochs=120, batch_size=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\bansi\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\skimage\\feature\\_hog.py:119: skimage_deprecation: Default value of `block_norm`==`L1` is deprecated and will be changed to `L2-Hys` in v0.15\n",
      "  'be changed to `L2-Hys` in v0.15', skimage_deprecation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11560,)\n",
      "(11560, 50)\n",
      "(11560, 40)\n"
     ]
    }
   ],
   "source": [
    "timage =  cv2.imread('F:/Desktop/ICT/sem_6/ML/train/new_faces/201501028_Anger.jpg')\n",
    "tgray = cv2.cvtColor(timage, cv2.COLOR_BGR2GRAY)  \n",
    "\n",
    "fd, hog_image = hg(image, orientations=10, pixels_per_cell=(16, 16),\n",
    "                        cells_per_block=(2, 2), visualise= True)\n",
    "\n",
    "print(fd.shape)\n",
    "# pca.fit(fd)\n",
    "\n",
    "fd = fd.reshape(-1,1)\n",
    "x = pca.transform(fd)\n",
    "x = x/100\n",
    "print(x.shape)\n",
    "\n",
    "# tNP = tNP.flatten()\n",
    "print((model.predict(x)).shape)\n",
    "# tresize = cv2.resize(tgray,(100,100),interpolation = cv2.INTER_AREA)   \n",
    "# tNP = np.array(tresize)\n",
    "# tNP = tNP.flatten()\n",
    "# x = pca.transform(tNP.reshape(1,10000))\n",
    "# print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
